{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from gensim.summarization import keywords\n",
    "from keybert import KeyBERT\n",
    "from rake_nltk import Rake\n",
    "import yake\n",
    "import json\n",
    "import spacy\n",
    "import pke\n",
    "import textstat\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. RAKE\n",
    "def rake_extractor(text):\n",
    "    \"\"\"\n",
    "    Uses Rake to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return r.get_ranked_phrases()[:10]\n",
    "\n",
    "# 2. YAKE\n",
    "def yake_extractor(text):\n",
    "    \"\"\"\n",
    "    Uses YAKE to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    keywords = yake.KeywordExtractor(lan=\"en\", n=3, windowsSize=3, top=10).extract_keywords(text)\n",
    "    results = []\n",
    "    for scored_keywords in keywords:\n",
    "        for keyword in scored_keywords:\n",
    "            if isinstance(keyword, str):\n",
    "                results.append(keyword) \n",
    "    return results \n",
    "\n",
    "\n",
    "# 3. PositionRank\n",
    "def position_rank_extractor(text):\n",
    "    \"\"\"\n",
    "    Uses PositionRank to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    # define the valid Part-of-Speeches to occur in the graph\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
    "    extractor = pke.unsupervised.PositionRank()\n",
    "    extractor.load_document(text, language='en')\n",
    "    extractor.candidate_selection(maximum_word_number=5)\n",
    "    # 4. weight the candidates using the sum of their word's scores that are\n",
    "    #    computed using random walk biaised with the position of the words\n",
    "    #    in the document. In the graph, nodes are words (nouns and\n",
    "    #    adjectives only) that are connected if they occur in a window of\n",
    "    #    3 words.\n",
    "    extractor.candidate_weighting(window=3, pos=pos)\n",
    "    # 5. get the 5-highest scored candidates as keyphrases\n",
    "    keyphrases = extractor.get_n_best(n=10)\n",
    "    results = []\n",
    "    for scored_keywords in keyphrases:\n",
    "        for keyword in scored_keywords:\n",
    "            if isinstance(keyword, str):\n",
    "                results.append(keyword) \n",
    "    return results \n",
    "\n",
    "# 4. SingleRank\n",
    "def single_rank_extractor(text):\n",
    "    \"\"\"\n",
    "    Uses SingleRank to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
    "    extractor = pke.unsupervised.SingleRank()\n",
    "    extractor.load_document(text, language='en')\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "    extractor.candidate_weighting(window=3, pos=pos)\n",
    "    keyphrases = extractor.get_n_best(n=10)\n",
    "    results = []\n",
    "    for scored_keywords in keyphrases:\n",
    "        for keyword in scored_keywords:\n",
    "            if isinstance(keyword, str):\n",
    "                results.append(keyword) \n",
    "    return results \n",
    "\n",
    "# 5. MultipartiteRank\n",
    "def multipartite_rank_extractor(text):\n",
    "    \"\"\"\n",
    "    Uses MultipartiteRank to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(text, language='en')\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1, threshold=0.74, method='average')\n",
    "    keyphrases = extractor.get_n_best(n=10)\n",
    "    results = []\n",
    "    for scored_keywords in keyphrases:\n",
    "        for keyword in scored_keywords:\n",
    "            if isinstance(keyword, str):\n",
    "                results.append(keyword) \n",
    "    return results\n",
    "\n",
    "# 6. TopicRank\n",
    "def topic_rank_extractor(text):\n",
    "    \"\"\"\n",
    "    Uses TopicRank to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    extractor.load_document(text, language='en')\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "    extractor.candidate_weighting()\n",
    "    keyphrases = extractor.get_n_best(n=10)\n",
    "    results = []\n",
    "    for scored_keywords in keyphrases:\n",
    "        for keyword in scored_keywords:\n",
    "            if isinstance(keyword, str):\n",
    "                results.append(keyword) \n",
    "    return results\n",
    "\n",
    "# 7. KeyBERT\n",
    "def keybert_extractor(text):\n",
    "    bert = KeyBERT()\n",
    "    \"\"\"\n",
    "    Uses KeyBERT to extract the top 5 keywords from a text\n",
    "    Arguments: text (str)\n",
    "    Returns: list of keywords (list)\n",
    "    \"\"\"\n",
    "    keywords = bert.extract_keywords(text, keyphrase_ngram_range=(1, 3), stop_words=\"english\", top_n=10)\n",
    "    results = []\n",
    "    for scored_keywords in keywords:\n",
    "        for keyword in scored_keywords:\n",
    "            if isinstance(keyword, str):\n",
    "                results.append(keyword)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filer():\n",
    "    def __init__(self, inputf):\n",
    "        self.inputf = inputf\n",
    "    \n",
    "    def write_file(self, data, outputf):\n",
    "        with open(outputf, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def read_file(self):\n",
    "        with open(self.inputf) as f:\n",
    "            data = json.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.summarizer = pipeline(\"summarization\", model=self.model)\n",
    "    \n",
    "    def get_summary(self, text):\n",
    "        words = text.split()\n",
    "        totalwords = len(words)\n",
    "        summary = self.summarizer(text, max_length = totalwords, do_sample=False)[0].get('summary_text')\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordExtractor():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def get_keywords(self, text):\n",
    "        if self.model == 'KEYBERT':\n",
    "            head = {'KEYBERT': keybert_extractor(text)}\n",
    "        elif self.model == 'TOPIC':\n",
    "            head = {'TOPIC RANK': topic_rank_extractor(text)} \n",
    "        elif self.model == 'MULTIPARTITE':\n",
    "            head = {'MULTIPARTITE RANK': multipartite_rank_extractor(text)}\n",
    "        elif self.model == 'SINGLE':\n",
    "            head = {'SINGLE RANK': single_rank_extractor(text)}\n",
    "        elif self.model == 'YAKE':\n",
    "            head = {'YAKE': yake_extractor(text)}\n",
    "        elif self.model == 'RAKE':\n",
    "            head = {'RAKE': rake_extractor(text)}\n",
    "        elif self.model == 'POSITION':\n",
    "            head = {'POSITION RANK': position_rank_extractor(text)}\n",
    "        else:\n",
    "            head = {}\n",
    "            \n",
    "        return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fulltext_summary(text, ex_summarizer):\n",
    "    summary = \"\"\n",
    "    for x in text.get('original').get('sections'):\n",
    "        summary += ex_summarizer.get_summary(x)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(json_data, summ, keyword):\n",
    "    ex_keyword = KeywordExtractor(keyword)\n",
    "    ex_summarizer = Summarizer(summ)\n",
    "    data = []\n",
    "    \n",
    "    for i in json_data:\n",
    "        s_abstract = ex_summarizer.get_summary(i.get('original').get('abstract'))\n",
    "        s_full = get_fulltext_summary(i, ex_summarizer)\n",
    "        s_article = ex_summarizer.get_summary(i.get('article').get('text'))\n",
    "        head = {\n",
    "            \"abstract\": {\n",
    "                \"title\": i.get('original').get('title'),\n",
    "                \"url\": i.get('original').get('url'),\n",
    "                \"abstract\": i.get('original').get('abstract'),\n",
    "                \"tokens\": len(nltk.word_tokenize(i.get('original').get('abstract'))),\n",
    "            },\n",
    "            \"full_text\": {\n",
    "                \"title\": i.get('original').get('title'),\n",
    "                \"url\": i.get('original').get('url'),\n",
    "                \"keywords\": i.get('original').get('keywords'),\n",
    "                \"text\": i.get('original').get('text'),\n",
    "                \"sections\": i.get('original').get('sections'),\n",
    "                \"tokens\": len(nltk.word_tokenize(i.get('original').get('text'))),\n",
    "            },\n",
    "            \"article\": {\n",
    "                \"title\": i.get('article').get('title'),\n",
    "                \"url\": i.get('article').get('url'),\n",
    "                \"keywords\": i.get('article').get('keywords'),\n",
    "                \"text\": i.get('article').get('text'),\n",
    "                \"tokens\": len(nltk.word_tokenize(i.get('article').get('text'))),\n",
    "            },\n",
    "            \"summaries\": {\n",
    "                \"abstract\": {\n",
    "                    \"text\": s_abstract,\n",
    "                    \"tokens\": len(nltk.word_tokenize(s_abstract)),\n",
    "                },\n",
    "                \"full_text\": {\n",
    "                    \"text\": s_full,\n",
    "                    \"tokens\": len(nltk.word_tokenize(s_full)),\n",
    "                },\n",
    "                \"article\": {\n",
    "                    \"text\": s_article,\n",
    "                    \"tokens\": len(nltk.word_tokenize(s_article)),\n",
    "                }\n",
    "            },\n",
    "            \"keywords\": {\n",
    "                \"abstract\": ex_keyword.get_keywords(i.get('original').get('abstract')),\n",
    "                \"full_text\": ex_keyword.get_keywords(i.get('original').get('text')),\n",
    "                \"article\": ex_keyword.get_keywords(i.get('article').get('text'))\n",
    "            }\n",
    "        }\n",
    "        data.append(head)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxmonteiro/miniconda3/envs/profextra/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "summ_models=['facebook/bart-large-cnn', 'sshleifer/distilbart-cnn-12-6', 'philschmid/bart-large-cnn-samsum', 'google/pegasus-large', 'sshleifer/distill-pegasus-cnn-16-4', '', '']\n",
    "keyword_models=['KEYBERT', 'YAKE', 'RAKE', 'POSITION', 'SINGLE', 'MULTIPARTITE', 'TOPIC']\n",
    "\n",
    "summ = 'facebook/bart-large-cnn'\n",
    "keyword = 'KEYBERT'\n",
    "\n",
    "filer = Filer('/home/dxmonteiro/Desktop/WORKSPACE/ProfExtra/input.json')\n",
    "json_data = filer.read_file()\n",
    "\n",
    "for x,j in zip(summ_models, keyword_models):\n",
    "    data = process_data(json_data, x, j)\n",
    "    sun = x.split('/')[1]\n",
    "    filer.write_file(data, f'/home/dxmonteiro/Desktop/WORKSPACE/ProfExtra/output_{sun}_{j}.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('profextra')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c053a61acfdbdf1174e3d410b8ae0c87c7d4854c4a9812804c3f7123709dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
