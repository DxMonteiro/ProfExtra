[
  {
    "abstract": {
      "title": "What Is Missing from Contemporary AI? The World",
      "url": "https://spj.sciencemag.org/journals/icomputing/2022/9847630/",
      "abstract": "In the past three years, we have witnessed the emergence of a new class of artificial intelligence systems so-called foundation models, which are characterised by very large machine learning models (with tens or hundreds of billions of parameters) trained using extremely large and broad data sets. Foundation models, it is argued, have competence in a broad range of tasks, which can be specialised for specific applications. Large language models, of which GPT-3 is perhaps the best known, are the most prominent example of current foundation models. While foundation models have demonstrated impressive capabilities in certain tasks\u2014natural language generation being the most obvious example\u2014I argue that because they are inherently disembodied, and they are limited with respect to what they have learned and what they can do. Foundation models are likely to be very useful in many applications: but they are not the end of the road in artificial intelligence.",
      "tokens": 165
    },
    "full_text": {
      "title": "What Is Missing from Contemporary AI? The World",
      "url": "https://spj.sciencemag.org/journals/icomputing/2022/9847630/",
      "keywords": [],
      "text": "Over the past 15 years, the speed of progress in artificial intelligence (AI) in general, and machine learning (ML) in particular, has repeatedly taken seasoned AI commentators like myself by surprise: we have had to continually recalibrate our expectations as to what is going to be possible and when. For example, at the start of the new millennium, it felt like practical automated translation tools were a still a remote prospect, and that much research was going to be required before this problem was really solved. But at some point over the past 15 years, this technology became routine\u2014this miracle of the modern age is now taken for granted. Time and time again this century, we have seen AI leap past benchmark problems: in 2011, IBM\u2019s Watson program beat expert human players in the notoriously difficult Jeopardy game show [1]; in 2013, DeepMind unveiled Atari-playing programs that learned to play a range of 8-bit video games just by repeatedly playing them, using no more than access to the game\u2019s video feed and current score [2]; in 2016, DeepMind effectively solved Go, in a series of matches against a human grand master which made world headlines [3]; they followed this up with progressively more startling achievements in computer game playing, with world-beating chess programs training themselves to play in a matter of hours [4]; and finally, in 2021, the AlphaFold system demonstrated an unprecedented level of performance in the fundamental protein folding problem [5]. It is generally accepted that these breakthroughs\u2014and the many others that I have not mentioned here\u2014are in part a consequence of scientific developments in machine learning (new machine learning architectures and refined training algorithms), but just as importantly, the use of large training data sets is coupled with extensive compute resources. While the fundamental machine learning algorithms underpinning the current resurgence of AI have been around for decades (gradient descent in particular), the latter has been only available in practice this century. It has become clear throughout the current wave of AI innovation that data and compute power really are fundamental to the success of AI techniques: the competence of AI models scales directly with their size, the resources used to train them, and the scale of training data. Richard S. Sutton, one of the world\u2019s leading reinforcement learning researchers, was so struck by this that he coined a term to describe it: the \u201cbitter lesson\u201d of AI is that advances in AI are dominated by the use of increasingly large data sets and increasingly greater compute resources [6]. When it comes to building successful machine learning models, it seems, might really is right. Given this lesson, it is then perhaps no surprise that we have seen a rush for scale in machine learning. AI researchers, seeking to steal an advantage on their competitors, have built ever-larger ML models, using ever more compute resources for training. And it seems to be working. Over the past three years, the rush to scale has led to the emergence of a new class of AI system: foundation models [7]. Foundation models are very large machine learning models, trained on extremely large and very broad data sets, using substantial compute resources. They mark a shift in emphasis away from AI systems that have very narrowly focused expertise, suitable only for one tiny problem or class of problems. The bet with foundation models is that their extensive and broad training leads to them learning useful competences across a range of areas, which can then be specialised for specific applications. While symbolic AI was predicated on the assumption that intelligence is primarily a problem of knowledge, foundation models are predicated on the assumption that intelligence is primarily a problem of data. Throw enough training data at big models, and hopefully, competence will arise. I am simplifying, of course\u2014but not much. The most prominent foundation model developed to date is GPT-3 [8]. Released in 2021, GPT-3 is the canonical example of a large language model (LLM). Its operation is best described by analogy. When you use your smartphone to write a text message or email, it has a feature that suggests the completions of your sentences\u2014thus, when you type \u201cI will be\u2026\u201d it uses your history of previous messages to predict that the next word will be \u201clate\u201d and suggests this to you as a possible completion of your message. Every time you type a message on your phone, you are training your phone so that it can make better suggestions. Large language models do something similar but on a vastly larger scale. GPT-3, for example, was trained on essentially the entirety of the English text available on the World Wide Web. GPT-3 has been shown to have unprecedented capabilities in natural language generation, being capable of generating extended pieces of very natural-sounding text. Of perhaps even more interest is that it also seems to have acquired some competence in common-sense reasoning\u2014one of the holy grails of AI research over the past 60 years. Many other large language models have appeared since GPT-3: one system attracting considerable attention at present is Google\u2019s LAMDA system [9], the state-of-the-art in serious chatbot technology. In June 2022, LAMDA attracted a lot of (unwelcome) publicity when a Google engineer by the name of Blake Lemoine was suspended from the company after he claimed the system was sentient. Whatever the validity of Blake\u2019s conclusions (I personally do not think this is a reasonable conclusion, and I think we still are a long way from sentient AI), it is clear that he was deeply impressed by LAMDA\u2019s ability to converse\u2014and with good reason. Foundation models are impressive and an important current development in the AI landscape. We will see many creative uses of them in the years ahead\u2014and no doubt there are fortunes to be made (and possibly business empires) on the back of them. But what do they really mean for progress in artificial intelligence? Is this it, and all that remains for AI now is scale, as one DeepMind researcher put it? For all that their achievements are to be lauded, I think there is one crucial respect in which most large ML models are greatly restricted\u2014and which limits their capabilities with respect to AI in general. And that is the world\u2014and in particular, the fact that foundation models simply have no experience of it. They are, ultimately, disembodied AI systems. This has implications both for their status as AI systems, but also, and more pragmatically, for their uses in the world. To better understand the first point, let us pause for a moment to think about what a large language model has learned. A large language model is trained by presenting to it a very large corpus of existing text written in some language\u2014say English. This corpus embodies knowledge about the words that we use to describe things in the world. Thus, for example, a large language model may well learn that \u201crain is wet,\u201d and if asked whether rain is wet or dry, will likely respond that rain is wet. But, unlike us, the model has never experienced wetness. The word \u201cwet\u201d to the language model is nothing more than a symbol, which is often used in connection with words like \u201crain\u201d and so on. Given this, does the model in any sense understand the concept of \u201cwetness\u201d? Well, in a linguistic sense, yes. GPT-3 can likely write you a plausible essay about wetness\u2014the pleasure of diving into the Spanish Mediterranean sea, the misery of drizzle on a dank English January afternoon, and so on. But that is a very impoverished conception of understanding. I do not think you can understand a concept like wet unless you have actually experienced it. The concept of \u201cwet,\u201d for me, means all the experiences I have ever had of being wet. That is how I understand the concept. But large language models like GPT-3 have not experienced anything in the real world. All they have experienced is a very large collection of symbols (their training data), which stand in certain relations to one another (the word \u201cwet\u201d used in connection with \u201crain\u201d, for example). At no point is there any grounding for these symbols\u2014no sense in which they are given meaning with respect to concepts that have actually been experienced in the world. There is a counter argument to this position, to the effect that at some point, if there is enough data\u2014enough descriptions of experiences, even without the actual experiences themselves\u2014then the lack of grounding ceases to matter. In this case, so the argument goes, a system might be able to reliably convince us that it really does understand wetness\u2014at which point is there any point in arguing about it anymore? From a purely practical perspective, I would tend to accept that point. After all, the fact that a system has not had any actual experiences in the world does not stop it from being useful; nor does it in fact stop it from being an expert (in a restricted sense) on the subject of those experiences. But from an AI perspective, it does rather throw in doubt the possibility that such a system has the same status as us with respect to issues such as understanding. To put this into conventional AI terms, I am sceptical that AI systems generated with this methodology can exhibit strong AI\u2014although I emphasise again, this does not mean they cannot be useful in their own right. A less philosophical concern about this approach to AI is that models with no experience of the world actually have AI capabilities that are greatly restricted. Many of the most important\u2014and challenging\u2014problems in AI are problems in the physical world. Many tasks in the physical world that we regard as not requiring intelligence in any meaningful sense (riding a bicycle, catching a ball, and cooking a meal) are highly nontrivial for robotics AI. Robots that have the full range of physical capabilities that a human has are a long, long way of\u2014arguably even more remote than AI systems that have the full range of intellectual capabilities. It is disappointing, I think, that so many of the AI systems about which we have become so excited in the past decade are not embodied in the world in any meaningful way. Of course, it is not hard to see why this is the case. The real world is harder\u2014much harder\u2014than simulated/virtual worlds like computer games. The problem is, as ML researcher Michael Littman put it, the real world just does not come in tidy data structures. And we cannot let a robot learn how to cook a meal by letting it experiment in our kitchen, as we can with computer games. Similarly for driverless cars: letting them loose on the roads to learn for themselves is a nonstarter. So, for all these reasons and more, researchers choose to build their models either in virtual worlds (such as computer games), or without any pretence of a world (large language models). And in this way, we are getting excited about a generation of AI systems that simply have no ability to operate in the single most important environment of all: our world. To be fair, there are some signs that this is changing. In May 2022, DeepMind announced the Gato system, a general purpose foundation model whose training data included large language corpus data like GPT-3, but which was also trained on robotic data\u2014it was capable of operating in an (admittedly very simple) physical environment [10]. Gato is an impressive achievement, and it is wonderful to see the first baby steps taken into the physical world by foundation models. But they are just baby steps: the challenges to overcome in making AI work in our world are at least as large\u2014and probably larger\u2014than those faced by making AI work in simulated environments. To paraphrase Winston Churchill, we are not looking at the end of the road in AI, but we may have reached the end of the beginning of the road.",
      "sections": [
        "Over the past 15 years, the speed of progress in artificial intelligence (AI) in general, and machine learning (ML) in particular, has repeatedly taken seasoned AI commentators like myself by surprise: we have had to continually recalibrate our expectations as to what is going to be possible and when. For example, at the start of the new millennium, it felt like practical automated translation tools were a still a remote prospect, and that much research was going to be required before this problem was really solved. But at some point over the past 15 years, this technology became routine\u2014this miracle of the modern age is now taken for granted. Time and time again this century, we have seen AI leap past benchmark problems: in 2011, IBM\u2019s Watson program beat expert human players in the notoriously difficult Jeopardy game show [1]; in 2013, DeepMind unveiled Atari-playing programs that learned to play a range of 8-bit video games just by repeatedly playing them, using no more than access to the game\u2019s video feed and current score [2]; in 2016, DeepMind effectively solved Go, in a series of matches against a human grand master which made world headlines [3]; they followed this up with progressively more startling achievements in computer game playing, with world-beating chess programs training themselves to play in a matter of hours [4]; and finally, in 2021, the AlphaFold system demonstrated an unprecedented level of performance in the fundamental protein folding problem [5]. It is generally accepted that these breakthroughs\u2014and the many others that I have not mentioned here\u2014are in part a consequence of scientific developments in machine learning (new machine learning architectures and refined training algorithms), but just as importantly, the use of large training data sets is coupled with extensive compute resources. While the fundamental machine learning algorithms underpinning the current resurgence of AI have been around for decades (gradient descent in particular), the latter has been only available in practice this century. It has become clear throughout the current wave of AI innovation that data and compute power really are fundamental to the success of AI techniques: the competence of AI models scales directly with their size, the resources used to train them, and the scale of training data. Richard S. Sutton, one of the world\u2019s leading reinforcement learning researchers, was so struck by this that he coined a term to describe it: the \u201cbitter lesson\u201d of AI is that advances in AI are dominated by the use of increasingly large data sets and increasingly greater compute resources [6]. When it comes to building successful machine learning models, it seems, might really is right. Given this lesson, it is then perhaps no surprise that we have seen a rush for scale in machine learning. AI researchers, seeking to steal an advantage on their competitors, have built ever-larger ML models, using ever more compute resources for training. And it seems to be working.",
        "Over the past three years, the rush to scale has led to the emergence of a new class of AI system: foundation models [7]. Foundation models are very large machine learning models, trained on extremely large and very broad data sets, using substantial compute resources. They mark a shift in emphasis away from AI systems that have very narrowly focused expertise, suitable only for one tiny problem or class of problems. The bet with foundation models is that their extensive and broad training leads to them learning useful competences across a range of areas, which can then be specialised for specific applications. While symbolic AI was predicated on the assumption that intelligence is primarily a problem of knowledge, foundation models are predicated on the assumption that intelligence is primarily a problem of data. Throw enough training data at big models, and hopefully, competence will arise. I am simplifying, of course\u2014but not much. The most prominent foundation model developed to date is GPT-3 [8]. Released in 2021, GPT-3 is the canonical example of a large language model (LLM). Its operation is best described by analogy. When you use your smartphone to write a text message or email, it has a feature that suggests the completions of your sentences\u2014thus, when you type \u201cI will be\u2026\u201d it uses your history of previous messages to predict that the next word will be \u201clate\u201d and suggests this to you as a possible completion of your message. Every time you type a message on your phone, you are training your phone so that it can make better suggestions. Large language models do something similar but on a vastly larger scale. GPT-3, for example, was trained on essentially the entirety of the English text available on the World Wide Web.",
        "GPT-3 has been shown to have unprecedented capabilities in natural language generation, being capable of generating extended pieces of very natural-sounding text. Of perhaps even more interest is that it also seems to have acquired some competence in common-sense reasoning\u2014one of the holy grails of AI research over the past 60 years. Many other large language models have appeared since GPT-3: one system attracting considerable attention at present is Google\u2019s LAMDA system [9], the state-of-the-art in serious chatbot technology. In June 2022, LAMDA attracted a lot of (unwelcome) publicity when a Google engineer by the name of Blake Lemoine was suspended from the company after he claimed the system was sentient. Whatever the validity of Blake\u2019s conclusions (I personally do not think this is a reasonable conclusion, and I think we still are a long way from sentient AI), it is clear that he was deeply impressed by LAMDA\u2019s ability to converse\u2014and with good reason. Foundation models are impressive and an important current development in the AI landscape. We will see many creative uses of them in the years ahead\u2014and no doubt there are fortunes to be made (and possibly business empires) on the back of them. But what do they really mean for progress in artificial intelligence? Is this it, and all that remains for AI now is scale, as one DeepMind researcher put it?",
        "For all that their achievements are to be lauded, I think there is one crucial respect in which most large ML models are greatly restricted\u2014and which limits their capabilities with respect to AI in general. And that is the world\u2014and in particular, the fact that foundation models simply have no experience of it. They are, ultimately, disembodied AI systems. This has implications both for their status as AI systems, but also, and more pragmatically, for their uses in the world. To better understand the first point, let us pause for a moment to think about what a large language model has learned. A large language model is trained by presenting to it a very large corpus of existing text written in some language\u2014say English. This corpus embodies knowledge about the words that we use to describe things in the world. Thus, for example, a large language model may well learn that \u201crain is wet,\u201d and if asked whether rain is wet or dry, will likely respond that rain is wet. But, unlike us, the model has never experienced wetness. The word \u201cwet\u201d to the language model is nothing more than a symbol, which is often used in connection with words like \u201crain\u201d and so on. Given this, does the model in any sense understand the concept of \u201cwetness\u201d? Well, in a linguistic sense, yes. GPT-3 can likely write you a plausible essay about wetness\u2014the pleasure of diving into the Spanish Mediterranean sea, the misery of drizzle on a dank English January afternoon, and so on. But that is a very impoverished conception of understanding. I do not think you can understand a concept like wet unless you have actually experienced it. The concept of \u201cwet,\u201d for me, means all the experiences I have ever had of being wet. That is how I understand the concept. But large language models like GPT-3 have not experienced anything in the real world. All they have experienced is a very large collection of symbols (their training data), which stand in certain relations to one another (the word \u201cwet\u201d used in connection with \u201crain\u201d, for example). At no point is there any grounding for these symbols\u2014no sense in which they are given meaning with respect to concepts that have actually been experienced in the world.",
        "There is a counter argument to this position, to the effect that at some point, if there is enough data\u2014enough descriptions of experiences, even without the actual experiences themselves\u2014then the lack of grounding ceases to matter. In this case, so the argument goes, a system might be able to reliably convince us that it really does understand wetness\u2014at which point is there any point in arguing about it anymore? From a purely practical perspective, I would tend to accept that point. After all, the fact that a system has not had any actual experiences in the world does not stop it from being useful; nor does it in fact stop it from being an expert (in a restricted sense) on the subject of those experiences. But from an AI perspective, it does rather throw in doubt the possibility that such a system has the same status as us with respect to issues such as understanding. To put this into conventional AI terms, I am sceptical that AI systems generated with this methodology can exhibit strong AI\u2014although I emphasise again, this does not mean they cannot be useful in their own right. A less philosophical concern about this approach to AI is that models with no experience of the world actually have AI capabilities that are greatly restricted. Many of the most important\u2014and challenging\u2014problems in AI are problems in the physical world. Many tasks in the physical world that we regard as not requiring intelligence in any meaningful sense (riding a bicycle, catching a ball, and cooking a meal) are highly nontrivial for robotics AI. Robots that have the full range of physical capabilities that a human has are a long, long way of\u2014arguably even more remote than AI systems that have the full range of intellectual capabilities.",
        "It is disappointing, I think, that so many of the AI systems about which we have become so excited in the past decade are not embodied in the world in any meaningful way. Of course, it is not hard to see why this is the case. The real world is harder\u2014much harder\u2014than simulated/virtual worlds like computer games. The problem is, as ML researcher Michael Littman put it, the real world just does not come in tidy data structures. And we cannot let a robot learn how to cook a meal by letting it experiment in our kitchen, as we can with computer games. Similarly for driverless cars: letting them loose on the roads to learn for themselves is a nonstarter. So, for all these reasons and more, researchers choose to build their models either in virtual worlds (such as computer games), or without any pretence of a world (large language models). And in this way, we are getting excited about a generation of AI systems that simply have no ability to operate in the single most important environment of all: our world. To be fair, there are some signs that this is changing. In May 2022, DeepMind announced the Gato system, a general purpose foundation model whose training data included large language corpus data like GPT-3, but which was also trained on robotic data\u2014it was capable of operating in an (admittedly very simple) physical environment [10]. Gato is an impressive achievement, and it is wonderful to see the first baby steps taken into the physical world by foundation models. But they are just baby steps: the challenges to overcome in making AI work in our world are at least as large\u2014and probably larger\u2014than those faced by making AI work in simulated environments. To paraphrase Winston Churchill, we are not looking at the end of the road in AI, but we may have reached the end of the beginning of the road."
      ],
      "tokens": 2283
    },
    "article": {
      "title": "Physical training is the next hurdle for artificial intelligence, researcher says",
      "url": "https://techxplore.com/news/2022-09-physical-hurdle-artificial-intelligence.html",
      "keywords": [],
      "text": "Let a million monkeys clack on a million typewriters for a million years and, the adage goes, they'll reproduce the works of Shakespeare. Give infinite monkeys infinite time, and they still will not appreciate the bard's poetic turn-of-phrase, even if they can type out the words. The same holds true for artificial intelligence (AI), according to Michael Woolridge, professor of computer science at the University of Oxford. The issue, he said, is not the processing power, but rather a lack of experience. His perspective was published on July 25 in Intelligent Computing. 'Over the past 15 years, the speed of progress in AI in general, and machine learning (ML) in particular, has repeatedly taken seasoned AI commentators like myself by surprise: we have had to continually recalibrate our expectations as to what is going to be possible and when,' Wooldridge said. 'For all that their achievements are to be lauded, I think there is one crucial respect in which most large ML models are greatly restricted: the world and the fact that the models simply have no experience of it.' Most ML models are built in virtual worlds, such as video games. They can train on massive datasets, but for physical applications, they are missing vital information. Wooldridge pointed to the AI underpinning autonomous vehicles as an example. 'Letting driverless cars loose on the roads to learn for themselves is a nonstarter, so for this and other reasons, researchers choose to build their models in virtual worlds,' Wooldridge said. 'And in this way, we are getting excited about a generation of AI systems that simply have no ability to operate in the single most important environment of all: our world.' Language AI models, on the other hand, are developed without a pretense of a world at all\u2014but still suffer from the same limitations. They have evolved, so to speak, from laughably terrible predictive texts to Google's LaMDA, which made headlines earlier this year when a now-former Google engineer claimed the AI was sentient. 'Whatever the validity of [the engineer's] conclusions, it was clear that he was deeply impressed by LaMDA's ability to converse\u2014and with good reason,' Wooldridge said, noting that he does not personally believe LaMDA is sentient, nor is AI near such a milestone. 'These foundational models demonstrate unprecedented capabilities in natural language generation, producing extended pieces of natural-sounding text. They also seem to have acquired some competence in common-sense reasoning, one of the holy grails of AI research over the past 60 years.' Such models are neural networks, feeding on enormous datasets and training to understand them. For example, GPT-3, a predecessor to LaMDA, trained on all of the English text available on the internet. The amount of training data combined with significant computing power makes the models akin to human brains, where they move past narrow tasks to begin recognizing patterns and make connections seemingly unrelated to the primary task. 'The bet with foundation models is that their extensive and broad training leads to useful competencies across a range of areas, which can then be specialized for specific applications,' Wooldridge said. 'While symbolic AI was predicated on the assumption that intelligence is primarily a problem of knowledge, foundation models are predicated on the assumption that intelligence is primarily a problem of data. To simplify, but not by much, throw enough training data at big models, and hopefully, competence will arise.' This 'might is right' approach scales the models larger to produce smarter AI, Wooldridge argued, but this ignores the physical know-how needed to truly advance AI. 'To be fair, there are some signs that this is changing,' Wooldridge said, pointing to the Gato system. Announced in May by DeepMind, the foundation model, trained on large language sets and on robotic data, could operate in a simple but physical environment. 'It is wonderful to see the first baby steps taken into the physical world by foundation models. But they are just baby steps: the challenges to overcome in making AI work in our world are at least as large\u2014and probably larger\u2014than those faced by making AI work in simulated environments.",
      "tokens": 787
    },
    "summaries": {
      "abstract": {
        "text": " Foundation models are characterised by very large machine learning models (with tens or hundreds of billions of parameters) trained using extremely large and broad data sets . Large language models, of which GPT-3 is perhaps the best known, are the most prominent example of current foundation models . Foundation models have demonstrated impressive capabilities in certain tasks, such as natural language generation .",
        "tokens": 68
      },
      "full_text": {
        "text": " In 2011, IBM\u2019s Watson program beat expert human players in the notoriously difficult Jeopardy game show . In 2013, DeepMind unveiled Atari-playing programs that learned to play a range of 8-bit video games just by repeatedly playing them . DeepMind effectively solved Go, in a series of matches against a human grand master . In 2021, AlphaFold system demonstrated an unprecedented level of performance in the fundamental protein folding problem . Foundation models are very large machine learning models trained on extremely large and very broad data sets . They mark a shift in emphasis away from AI systems that have very narrowly focused expertise, suitable only for one tiny problem or class of problems . The bet with foundation models is that their extensive and broad training leads to them learning useful competences across a range of areas, which can then be specialised for specific applications . GPT-3 has been shown to have unprecedented capabilities in natural language generation, being capable of generating extended pieces of very natural-sounding text . Of perhaps even more interest is that it also seems to have acquired some competence in common-sense reasoning . Google\u2019s LAMDA system attracting considerable attention at present is the state-of-the-art in serious chatbot technology . A large language model is trained by presenting to it a very large corpus of existing text written in some language . This corpus embodies knowledge about the words that we use to describe things in the world . But, unlike us, the model has never experienced wetness . All they have experienced is a collection of symbols (their training data), which stand in certain relations to one another . Many tasks in the physical world that we regard as not requiring intelligence in any meaningful sense (riding a bicycle, catching a ball, and cooking a meal) are highly nontrivial for robotics AI . I am sceptical that AI systems generated with this methodology can exhibit strong AI\u2014 although I emphasise again, this does not mean they cannot be useful in their own right . It is disappointing that so many of the AI systems about which we have become so excited in the past decade are not embodied in the world in any meaningful way . The real world is harder\u2014much harder\u2014than simulated/virtual worlds like computer games . Researchers choose to build their models either in virtual worlds (such as computer games), or without any pretence of a world (large language models)",
        "tokens": 436
      },
      "article": {
        "text": " Michael Wooldridge, professor of computer science at the University of Oxford, says most AI models are built in virtual worlds, such as video games . He says the speed of progress in AI in general, and machine learning (ML) in particular, has repeatedly taken seasoned AI commentators by surprise . 'We have had to continually recalibrate our expectations as to what is going to be possible and when,' he said .",
        "tokens": 80
      }
    },
    "keywords": {
      "abstract": {
        "YAKE": [
          "hundreds of billions",
          "billions of parameters",
          "broad data sets",
          "large machine learning",
          "systems so-called foundation",
          "machine learning models",
          "intelligence systems so-called",
          "past three years",
          "trained using extremely",
          "data sets"
        ]
      },
      "full_text": {
        "YAKE": [
          "large language model",
          "models",
          "large language",
          "world",
          "foundation models",
          "large",
          "machine learning",
          "machine learning models",
          "language model",
          "data"
        ]
      },
      "article": {
        "YAKE": [
          "million monkeys clack",
          "million typewriters",
          "million monkeys",
          "models",
          "Wooldridge",
          "million",
          "infinite monkeys infinite",
          "million years",
          "monkeys infinite time",
          "Give infinite monkeys"
        ]
      }
    }
  }
]
