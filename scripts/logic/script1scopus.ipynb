{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Load Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from Tools.keyworder import Keyworder\n",
    "from Tools.languager import Languager\n",
    "from Tools.sentimenter import Sentimenter\n",
    "from Tools.summarizer import Summarizer\n",
    "from Tools.meaninger import Meaninger\n",
    "from Tools.filer import Filer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = '/home/dxmonteiro/Desktop/WORKSPACE/ProfExtra/scripts/data/humanidades_digitais_scopus.csv'\n",
    "\n",
    "title = 0\n",
    "citations = 1\n",
    "doi = 2\n",
    "link = 3\n",
    "abstract = 4\n",
    "keywords = [5,7,9,11,13,15,17,19]\n",
    "authors = [21,22,23,24,25,26,27,28,29,30,31,32,33,34,35]\n",
    "\n",
    "list_papers = []\n",
    "list_authors = []\n",
    "list_keywords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2b. Aux Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_list(elementa, list_authors, citations):\n",
    "  k = 0\n",
    "  for check in list_authors:\n",
    "    if check.get('name') == elementa:\n",
    "      author = {\n",
    "        'name': elementa,\n",
    "        'frequence': check.get('frequence') + 1,\n",
    "        'citations': check.get('citations') + citations\n",
    "      }\n",
    "      list_authors[k] = author\n",
    "      return True\n",
    "    k += 1\n",
    "  return False\n",
    "\n",
    "def get_values(real_authors, list_authors, citations):\n",
    "  for elementa in real_authors:\n",
    "    if not update_list(elementa, list_authors, citations):\n",
    "      author = {\n",
    "        'name': elementa,\n",
    "        'frequence': 1,\n",
    "        'citations': citations\n",
    "      }\n",
    "      list_authors.append(author)\n",
    "  return list_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2a. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input, 'r') as file:\n",
    "  csvreader = csv.reader(file)\n",
    "  next(csvreader)\n",
    "  for row in csvreader:\n",
    "    if not not row[citations]:\n",
    "      nplist = np.array(row)\n",
    "      real_authors = list(filter(None, nplist[authors]))\n",
    "      real_keywords = list(filter(None, nplist[keywords]))\n",
    "      new_cit = int(row[citations])\n",
    "      paper = {\n",
    "          'doi': nplist[doi],\n",
    "          'title': nplist[title],\n",
    "          'abstract': nplist[abstract],\n",
    "          'link': nplist[link],\n",
    "          'citations': new_cit,\n",
    "          'keywords': real_keywords,\n",
    "          'authors': real_authors\n",
    "      }\n",
    "      list_papers.append(paper)\n",
    "      list_authors = get_values(real_authors, list_authors, new_cit)\n",
    "      list_keywords = get_values(real_keywords, list_keywords, new_cit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Extract TOP10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_papers.sort(key=lambda x: x.get('citations'), reverse=True)\n",
    "top_10_papers = list_papers[:10]\n",
    "print(top_10_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_authors.sort(key=lambda x: x.get('citations'), reverse=True)\n",
    "top_10_authors = list_authors[:10]\n",
    "print(top_10_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keywords.sort(key=lambda x: x.get('citations'), reverse=True)\n",
    "top_10_keywords = list_keywords[:10]\n",
    "print(top_10_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Save TOP10s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filer = Filer('')\n",
    "\n",
    "top10s = {\n",
    "    'TOP10_PAPERS': top_10_papers,\n",
    "    'TOP10_AUTHORS': top_10_authors,\n",
    "    'TOP10_KEYWORDS': top_10_keywords\n",
    "}\n",
    "\n",
    "filer.write_file(top10s, '/home/dxmonteiro/Desktop/WORKSPACE/ProfExtra/scripts/data/top10.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Var Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_summ = ['facebook/bart-large-cnn',\n",
    "                'sshleifer/distilbart-cnn-12-6', 'philschmid/bart-large-cnn-samsum' ]\n",
    "\n",
    "already_key = ['KEYBERT', 'YAKE', 'RAKE']\n",
    "\n",
    "already_senti = ['cardiffnlp/twitter-roberta-base-sentiment',\n",
    "                 'finiteautomata/bertweet-base-sentiment-analysis', 'ProsusAI/finbert']\n",
    "\n",
    "\n",
    "summ_models = ['google/pegasus-large', 'sshleifer/distill-pegasus-cnn-16-4','google/bigbird-pegasus-large-bigpatent','csebuetnlp/mT5_multilingual_XLSum']\n",
    "keyword_models = ['POSITION', 'SINGLE', 'MULTIPARTITE', 'TOPIC']\n",
    "senti_models = ['pysentimiento/robertuito-sentiment-analysis', 'Seethal/sentiment_analysis_generic_dataset', 'unitary/toxic-bert', 'j-hartmann/emotion-english-distilroberta-base']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Paper Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(summ, key, senti, top_10_papers):\n",
    "    keyworder = Keyworder(key)\n",
    "    summarizer = Summarizer(summ)\n",
    "    sentimenter = Sentimenter(senti)\n",
    "    languager = Languager('en_core_web_sm')\n",
    "    meaninger = Meaninger('en_core_web_sm')\n",
    "    data = []\n",
    "    \n",
    "    for paper in top_10_papers:\n",
    "        \n",
    "        abstract = str(paper.get('abstract'))\n",
    "        \n",
    "        original_abstract = {\n",
    "            \"text\": abstract,\n",
    "            \"num_chars\": len(abstract),\n",
    "            \"num_words\": languager.num_words(abstract),\n",
    "            \"unique_words\": languager.unique_words(abstract),\n",
    "            \"points\": languager.points(abstract),\n",
    "            \"word_analysis\": languager.word_analysis(abstract),\n",
    "            \"sentiment_analysis\": sentimenter.sentiment_analysis(abstract)\n",
    "        }\n",
    "        \n",
    "        print(original_abstract)\n",
    "        \n",
    "        summarized_text = summarizer.get_summary(abstract)\n",
    "        \n",
    "        summarized_abstract = {\n",
    "            \"summarized_text\": summarized_text,\n",
    "            \"num_chars\": len(summarized_text),\n",
    "            \"num_words\": languager.num_words(summarized_text),\n",
    "            \"unique_words\": languager.unique_words(summarized_text),\n",
    "            \"points\": languager.points(summarized_text),\n",
    "            \"word_analysis\": languager.word_analysis(summarized_text),\n",
    "            \"sentiment_analysis\": sentimenter.sentiment_analysis(summarized_text),\n",
    "        }\n",
    "        \n",
    "        print(summarized_abstract)\n",
    "        \n",
    "        auto_keywords = keyworder.get_keywords(abstract)\n",
    "        \n",
    "        print(auto_keywords)\n",
    "        \n",
    "        manual_keywords = meaninger.get_all_meanings(abstract, paper.get('keywords'))\n",
    "        \n",
    "        new_auto_keys = meaninger.get_all_meanings(abstract, auto_keywords)\n",
    "            \n",
    "        paper = {\n",
    "          'doi': paper.get('doi'),\n",
    "          'title': paper.get('title'),\n",
    "          'link': paper.get('link'),\n",
    "          'citations': paper.get('citations'),\n",
    "          'authors': paper.get('authors'),\n",
    "          'original_abstract': original_abstract,\n",
    "          'summarized_abstract': summarized_abstract,\n",
    "          'author_keywords': manual_keywords,\n",
    "          'automatic_keywords': new_auto_keys\n",
    "        }\n",
    "        data.append(paper)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, k, l in zip(summ_models, keyword_models, senti_models):\n",
    "    data = process_data(s, k, l, top_10_papers)\n",
    "    sumi = s.split('/')[1]\n",
    "    senti = l.split('/')[1]\n",
    "    filer.write_file(\n",
    "        data, f'/home/dxmonteiro/Desktop/WORKSPACE/ProfExtra/scripts/data/output_{sumi}_{k}_{senti}.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('profextra')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c053a61acfdbdf1174e3d410b8ae0c87c7d4854c4a9812804c3f7123709dc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
